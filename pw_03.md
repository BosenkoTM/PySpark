# Практическая работа №3. Построение сквозного ML-пайплайна на больших данных с помощью Spark MLlib

---

## Цель работы
Научиться строить полные конвейеры машинного обучения (ML Pipelines) на платформе Apache Spark. Студенты пройдут путь от загрузки «сырых» больших данных и конструирования признаков (Feature Engineering) до обучения модели, оценки её качества и интерпретации результатов для решения бизнес-задач компании (на примере фитнес-индустрии).

**Бизнес-кейс.** Вы — Data Scientist в компании HealthTech (аналог Endomondo/Strava). Ваша задача — разработать прогностические модели для улучшения удержания клиентов, персонализации предложений и автоматической классификации активности.

**Задачи:**
-  Настроить среду разработки (Spark + Jupyter).
-  Выполнить предобработку данных: очистка, работа со сложными типами (массивы, timestamps).
-  Реализовать **Feature Engineering**: преобразование категориальных признаков, векторизация, нормализация.
-  Построить и обучить модель машинного обучения с использованием библиотеки **Spark MLlib**.
-  Оценить качество модели техническими метриками (ROC-AUC, RMSE, Silhouette) и перевести их в бизнес-показатели.

---

## Необходимое ПО и данные
Работа выполняется на основе конфигурации **`ds_mgpu_Hadoop3+spark_3_4`** (или в Google Colab).

*   **Датасет:** `endomondoHR.json` (Данные о тренировках пользователей: пульс, скорость, высота, вид спорта, пол).
*   **Источник:** [Скачать данные](https://drive.google.com/file/d/1yiAp1fFDy3wSqUR0X_btCZPtuczbLwCe/view?usp=drive_link) или [зеркало](https://disk.yandex.ru/d/d80nsbNoP6F2vA).

**Инструкция по запуску (Локально/Docker):**
-  Клонировать репозиторий: `git clone https://github.com/BosenkoTM/PySpark.git`
-  Поместить `endomondoHR.json` в папку `data`.
-  Запустить контейнер: `sudo docker compose up`
-  Перейти в Jupyter Lab: `http://localhost:10000/lab`

---

## Методические указания. Структура ML-пайплайна

В работе необходимо использовать объектно-ориентированный подход библиотеки `pyspark.ml`.

### Шаг 1. Подготовка данных (Data Preparation)
Загрузка JSON, фильтрация `null` значений, преобразование типов.
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, size

spark = SparkSession.builder.appName("EndomondoML").getOrCreate()
df = spark.read.json("data/endomondoHR.json")
# Пример фильтрации
df_clean = df.filter(size(col("heart_rate")) > 0).dropna(subset=["sport", "gender"])
```

### Шаг 2. Конструирование признаков (Feature Engineering)
Spark MLlib требует, чтобы входные данные были в формате векторов.
*   **StringIndexer / OneHotEncoder:** Для категориальных полей (`sport`, `gender`).
*   **SQLTransformer / UDF:** Для извлечения признаков из массивов (например, средний пульс, макс. высота).
*   **VectorAssembler:** Для объединения всех признаков в единый вектор `features`.

### Шаг 3. Обучение модели (Modeling)
Выбор алгоритма в зависимости от задачи (Классификация, Регрессия, Кластеризация).
```python
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml import Pipeline

rf = RandomForestClassifier(labelCol="label", featuresCol="features", numTrees=10)
pipeline = Pipeline(stages=[indexer, encoder, assembler, rf])
model = pipeline.fit(train_data)
```

### Шаг 4. Оценка и интерпретация (Evaluation)
Использование `BinaryClassificationEvaluator`, `RegressionEvaluator` или `ClusteringEvaluator`. Интерпретация важности признаков (`featureImportances`).

---

## Варианты индивидуальных заданий

В каждом варианте необходимо выполнить три задания, составляющих единый кейс.
*   **Задание 1 (Data & Features).** Подготовка датасета и создание специфических признаков.
*   **Задание 2 (Model Training).** Обучение указанной модели MLlib.
*   **Задание 3 (Business Metrics).** Оценка модели и бизнес-вывод.

**Вариант выбирается согласно номеру студента.**

Задания на образовательном портале [IT-Adaptive](https://envlab.ru/mod/assign/view.php?id=469)
---

## Требования к отчетности

Работа сдается в виде ссылки на публичный репозиторий (GitHub/GitVerse/GitLab).

**Состав репозитория:**
-  **`README.md`**:
    *   Описание задачи, номер варианта.
    *   Инструкция по запуску (какой Docker образ, какие библиотеки).
    *   Ссылка на источник данных.
-  **`lab_03_ml.ipynb`**:
    *   Основной ноутбук с кодом.
    *   Код должен быть разбит на логические блоки (Setup -> ETL -> Features -> ML -> Eval).
    *   Обязательно использование `pyspark.ml.Pipeline`.
-  **`Report.md`**:
    *   **Введение:** Цель построения модели.
    *   **Предобработка:** Какие признаки созданы, как обработаны пропуски (с примерами кода/скриншотами).
    *   **Моделирование:** Параметры модели, процесс обучения.
    *   **Результаты:** Таблицы с метриками (Accuracy, RMSE и др.), графики (ROC Curve, Cluster Plot).
    *   **Бизнес-вывод:** Ответ на вопрос из Задания 3. Как эта модель принесет деньги или пользу компании?

---

## Критерии оценки

| Категория | Критерий | Баллы |
| :--- | :--- | :--- |
| **1. Техническая реализация (Spark Core & SQL)** | **3 баллов** | |
| | Данные корректно загружены, очищены от null/выбросов. | 1 |
| | Выполнена сложная обработка массивов/дат с помощью UDF или встроенных функций Spark. | 2 |
| **2. Машинное обучение (Spark MLlib)** | **4 баллов** | |
| | Корректно применены Feature Transformers (`VectorAssembler`, `StringIndexer`, `Scaler` и др.). | 1 |
| | Использован `Pipeline` API. Данные разделены на Train/Test. Модель обучена без ошибок. | 2 |
| | Произведена валидация модели на тестовой выборке с использованием соответствующих Evaluators. | 1 |
| **3. Аналитика и Бизнес-ценность** | **2 баллов** | |
| | Метрики качества интерпретированы корректно (не просто "Accuracy=0.8", а "Мы угадываем 8 из 10..."). | 1 |
| | Сделан обоснованный бизнес-вывод, связывающий работу модели с прибылью/эффективностью продукта. | 1 |
| **4. Оформление** | **1 балла** | |
| | Репозиторий и отчет оформлены согласно требованиям. Код чистый и документированный. | 1 |
| **Итого** | | **10** |

```


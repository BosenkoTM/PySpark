## Анализ и визуализация больших данных. Машинное обучение на больших данных с использованием Apache Spark MLlib
#### Цель и задачи работы:
- Познакомиться с понятием «большие данные» и способами их обработки;
- Познакомиться с инструментом `Apache Spark` и возможностями, которые он предоставляет для обработки больших данных.
- Получить навыки выполнения разведочного анализа данных использованием `pyspark`.

#### Порядок выполнения работы:
1. Запустите конфигурацию `ds_mgpu_Hadoop3+spark_3_4` [скачать](http://95.31.0.249/moodle/mod/folder/view.php?id=1335) или запустить [файл](advanced_pyspark_for_exploratory_data_analysis.ipynb)  в colab.
2. Клонировать текущий репозиторий на компьютер в случае работы локально в Docker:
> `$ git clone https://github.com/BosenkoTM/PySpark.git`
3. Скачайте датасет, расположенный по адресу `https://drive.google.com/file/d/1yiAp1fFDy3wSqUR0X_btCZPtuczbLwCe/view?usp=drive_link`. Распакуйте его и поместите файл `endomondoHR.json` в директорию `data` проекта (если директория отсутствует, создайте ее).
4. Запустите докер-контейнер со средой разработки и инструментом `Apache Spark` и дождитесь завершения его работы.
> `$ sudo docker compose up`
5. Откройте браузер и перейдите по адресу: http://localhost:10000/lab
6. Откройте в браузере файл ноутбука с примером разведочного анализа [advanced_pyspark_for_exploratory_data_analysis_student.ipynb](https://github.com/BosenkoTM/PySpark/blob/main/advanced_pyspark_for_exploratory_data_analysis_student.ipynb). Изучите этот файл и запустите его.
7. **Рассмотрите датасет для дальнейшего анализа согласно варианта**. 

**Обратите внимание:**
* Для сокращения расчетного времени можно обрабатывать только часть датасета;
* **Плагиат-работы не принимаются!**.



#### Список теоретических вопросов к отчету:
1. Фреймворк обработки больших данных `Apache Spark`, его назначение, функции и отличия от `Hadoop MapReduce`.
2. Понятие устойчивого распределенного набора данных (RDD). Понятие раздела RDD (partition). Способы создания RDD. Трансформации (transformations) и действия (actions). Кэширование (cache) данных в Spark.
3. RDD и PairRDD: понятие, назначение. Основные трансформации (transformations) и действия (actions) над ними.
4. Реализация концепции MapReduce в фреймворке Spark. Функции map, flatMap, mapValues, mapPartitions, reduce, reduceByKey.
5. Модели запуска Spark-приложений (YARN, Standalone, Kubernetes). Понятие драйвера (driver) и исполнителей (executors). Понятия задания (job), этапа (stage) и задачи (task). Модель ленивых вычислений (lazy) и ее применение в Spark. Понятие ориентированного ациклического графа (Directed Acyclic Graph, DAG).
6. Операция перемешивания данных (shuffle): причины возникновения, влияние на производительность. Класс Partitioner. Пути повышения производительности.
7. Понятие датафрейма в Spark. Основные операции над датафреймами. Скалярные функции, агрегирующие функции, оконные функции. Оптимизация запросов. Catalyst.
8. Клиентский (client) и кластерный (cluster) режимы работы Apache Spark.


#### Индивидуальные задания

| Вариант | Задание 1 (Интерпретация)                                                                               | Задание 2 (Интерпретация)                                                                                                   | Задание 3 (Интерпретация)                                                                                                                               | Задание 4 (Практика PySpark/Python)                                                                                                                               | Задание 5 (MLlib Концепция)                                                                                                                                                              |
| :------ | :------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1       | Опишите структуру данных (схему), представленную в разделе 2. Какие типы данных показались вам необычными для бизнес-анализа и почему? | Интерпретируйте диаграммы (круговую и столбчатую) из раздела 5, показывающие топ-5 видов спорта. Какие выводы для бизнеса можно сделать?        | Объясните, почему обнаружение пропущенных значений и аномальных нулей (раздел 3) важно перед началом анализа. Какие риски несет их игнорирование? | **Напишите код PySpark**, чтобы подсчитать общее количество уникальных пользователей (`userId`) в датафрейме `df`.                                                | Представьте, что Endomondo хочет рекомендовать пользователям новые виды спорта. Какие данные из проанализированных (пол, текущие виды спорта, частота тренировок) можно было бы использовать как признаки (features) для модели MLlib, предсказывающей интерес к новому виду спорта? Какова была бы целевая переменная? |
| 2       | Проанализируйте вывод команды `df.describe()` в разделе 2. Какие предварительные инсайты о пользователях и их активностях можно получить? | Интерпретируйте stacked bar chart "Список всех занятий в зависимости от пола" (раздел 6). В каких видах спорта наблюдается наибольший гендерный дисбаланс? | Зачем нужны UDF (User Defined Functions), описанные в разделе 7? Приведите пример бизнес-задачи, где может понадобиться создать новую метрику с помощью UDF. | **Напишите код PySpark**, чтобы отфильтровать датафрейм `df`, оставив только те строки, где вид спорта (`sport`) равен 'running'.                                 | Опишите, как можно было бы использовать Spark MLlib для задачи классификации: предсказать пол (`gender`) пользователя на основе данных о его тренировках (виды спорта, продолжительность, время старта). Какие признаки кажутся наиболее важными?                                                                  |
| 3       | На основе сводной таблицы в разделе 3 (`sum_dfs[0]`), сформулируйте 3 ключевых показателя (KPI), которые могли бы отслеживать менеджеры продукта Endomondo. | Изучите таблицу и диаграммы топ-5 видов спорта по *количеству тренировок* (раздел 6). Сравните с топ-5 по *количеству пользователей* (раздел 5). Есть ли различия и о чем они говорят? | Интерпретируйте гистограммы распределения количества рекордов за тренировку по видам спорта (раздел 5, конец). Для каких видов спорта характерны очень короткие/длинные сессии записей? | **Напишите код PySpark**, чтобы выбрать только столбцы `userId`, `gender` и `sport` из исходного датафрейма `df`.                                                | Какую задачу машинного обучения (классификация, регрессия, кластеризация) можно решить с помощью Spark MLlib, используя столбец `PerWorkoutRecordCount`? Например, можно ли предсказать, будет ли следующая тренировка "длинной" по количеству записей? Какие признаки для этого понадобятся?                         |
| 4       | Объясните своими словами концепцию "Ленивой оценки" (Lazy Evaluation) в Spark (раздел 4). Почему это может быть важно для аналитика, работающего с большими данными, даже если он не пишет код? | Проанализируйте boxplot "Sports count by gender" (раздел 6). Что он говорит о распределении количества видов спорта, которыми занимаются мужчины и женщины? | Интерпретируйте гистограммы распределения продолжительности тренировок по видам спорта (раздел 7). Какие бизнес-гипотезы можно выдвинуть на основе этих данных (например, о пользовательских привычках)? | **Напишите код PySpark**, чтобы добавить новый столбец `has_speed_data` в `df`, который будет равен `True`, если массив `speed` не пустой, и `False` в противном случае (используйте функцию `size`). | Основываясь на анализе продолжительности тренировок (столбец `duration`) и времени начала (`workout_start_time`), опишите, как можно было бы построить регрессионную модель в Spark MLlib для прогнозирования продолжительности следующей тренировки пользователя. Какие признаки (features) вы бы использовали?         |
| 5       | В разделе 3 было принято решение удалить тренировки с менее чем 50 записями. Оцените это решение с точки зрения бизнес-аналитика. Какие плюсы и минусы? | Изучите таблицу `interval_statistic_df` (раздел 8). Объясните значения для строк 'mean interval' и '50th percentile interval' для вида спорта 'running'. Что означают эти метрики? | Проанализируйте гистограммы распределения времени начала тренировки (раздел 8). Для каких видов спорта характерен бимодальный (утро/вечер) паттерн? Какие выводы можно сделать о стиле жизни пользователей? | **Напишите код PySpark**, чтобы отфильтровать `df`, оставив только тренировки, где количество записей (`PerWorkoutRecordCount`) больше или равно 50.             | Как анализ интервалов записи данных (`interval`, раздел 8) может помочь в подготовке данных для ML? Можно ли использовать статистику интервалов (среднее, медиана, stddev) как признаки для модели MLlib, например, для предсказания вероятности сбоя записи данных во время тренировки?                  |
| 6       | Какие столбцы в исходных данных являются массивами (согласно разделу 2)? Приведите пример бизнес-вопроса, для ответа на который потребуется анализ данных внутри одного из этих массивов. | Интерпретируйте диаграмму `interval_statistic_df` (раздел 8). Почему для 'max interval' и 'stdev interval' используется отдельная правая ось Y? Что это говорит о данных? | Опишите цель процесса сэмплирования данных, показанного в разделе 8 (`sampling_data`). В каких ситуациях бизнес-аналитику может потребоваться работать с выборкой, а не со всем набором данных? | **Напишите код PySpark**, чтобы сгруппировать `df` по виду спорта (`sport`) и посчитать среднее количество записей за тренировку (`PerWorkoutRecordCount`) для каждого вида спорта. | Предложите задачу кластеризации с использованием Spark MLlib (например, K-Means) для группировки видов спорта (`sport`) на основе их характеристик (средняя продолжительность, среднее количество записей, типичное время начала). Какие признаки нужно было бы подготовить?                               |
| 7       | Проанализируйте таблицу `gender_user_activity_count` (раздел 3). Рассчитайте среднее количество тренировок на одного пользователя для каждого пола. Что это говорит о вовлеченности? | Сравните столбцы '25th percentile interval' и '75th percentile interval' для 'cycling' и 'walking' в таблице `interval_statistic_df` (раздел 8). Что можно сказать о стабильности записи данных для этих активностей? | Интерпретируйте графики необработанного пульса по нормированному времени для 'running' и 'swimming' (раздел 8). Какие различия в динамике пульса вы видите? Какие факторы могут на это влиять? | **Напишите код PySpark**, чтобы найти 5 самых частых видов спорта (`sport`) в датафрейме `df`, отсортировав их по убыванию количества тренировок.                  | Как данные о пульсе (`heart_rate`) могут быть использованы для ML в Spark MLlib? Опишите идею модели, которая могла бы предсказывать вид спорта (`sport`) на основе временного ряда пульса во время тренировки. Какие шаги по обработке признаков (feature engineering) из массива `heart_rate` могли бы понадобиться? |
| 8       | В разделе 7 были созданы новые столбцы (`date_time`, `workout_start_time`, `duration`, `interval`) с помощью UDF. Какие новые бизнес-вопросы стало возможно исследовать благодаря этим столбцам? Приведите 2 примера. | Объясните, что показывает метрика '95th percentile interval' в таблице `interval_statistic_df` (раздел 8). Почему она может быть более информативной, чем 'max interval' при анализе типичного поведения? | Изучите 3D-графики траекторий тренировок (раздел 8). Какую ценность для бизнеса (например, для разработки фич приложения) может нести анализ таких траекторий? | **Напишите код PySpark**, чтобы применить существующую UDF `udf_get_duration` к столбцу `date_time` (предполагая, что он уже создан) и получить столбец `duration_minutes`. | Опишите, как можно использовать данные о траектории (`latitude`, `longitude`, `altitude`) для задачи классификации в Spark MLlib, например, для определения типа местности (город, парк, горы) во время тренировки. Какие агрегированные признаки из этих массивов можно было бы создать?                       |
| 9       | На основе информации из раздела 3, оцените полноту данных по полу (`gender`). Как проблема с 'unknown' гендером может повлиять на выводы анализа? Предложите варианты решения. | Интерпретируйте вывод `user_more_sports_df.describe()` (раздел 6). Что можно сказать о среднем и максимальном количестве видов спорта, которыми увлекаются пользователи из этой группы? | Для чего рассчитывался столбец `normalized_date_time` в разделе 8 перед построением графиков пульса? Какую проблему это решает при сравнении разных тренировок? | **Напишите код PySpark**, чтобы подсчитать количество тренировок для каждого значения в столбце `gender` (male, female, unknown).                                    | Можно ли использовать Spark MLlib для предсказания, будет ли пользователь заниматься *более чем одним* видом спорта? Какая это будет задача (классификация/регрессия)? Какие признаки пользователя (пол, возраст - если бы был, история активностей) могли бы быть полезны?                                      |
| 10      | Представьте, что вы должны подготовить отчет для маркетингового отдела. Какие 3 ключевых вывода из всей практической работы вы бы включили в первую очередь и почему? | Изучите код и результат создания `highest_sport_users_df_renamed` (раздел 5), где добавлена категория 'others'. Почему важно учитывать категорию 'others' при анализе и визуализации? | Проанализируйте 3D-графики тренировок (раздел 8). Какие из представленных видов спорта, судя по графикам, включают значительные изменения высоты (`altitude`)? | **Напишите код PySpark**, чтобы найти максимальную продолжительность тренировки (`duration`, предполагая, что столбец уже создан) во всем датасете `df`.               | Как знание о перепаде высот (`altitude`) во время тренировки может быть использовано в ML? Предложите идею модели в Spark MLlib, которая использует изменение высоты как признак, например, для оценки сложности тренировки (регрессия) или для классификации типа активности (бег по холмам vs равнине).      |
| 11      | Опишите данные, которые содержатся в столбцах `latitude`, `longitude`, `altitude`. Для каких бизнес-задач (кроме визуализации маршрута) эти данные могут быть полезны? | Проанализируйте распределение 'workout_start_time' для 'hiking' (раздел 8). Чем оно отличается от распределения для 'running'? Какие гипотезы это может породить? | Что означает столбец `PerWorkoutRecordCount` (раздел 3)? Как его можно использовать для оценки качества или характера тренировки? | **Напишите код PySpark**, чтобы отфильтровать `df`, оставив только те тренировки, где в массиве `altitude` есть хотя бы одно значение больше 1000 (используйте UDF или `array_max` / `exists`). | Опишите, как можно подготовить данные `latitude` и `longitude` для использования в модели кластеризации Spark MLlib, чтобы найти популярные зоны для тренировок (например, парки, стадионы). Какие преобразования данных (например, геохеширование) могут понадобиться?                                  |
| 12      | Какие виды спорта, согласно анализу в разделах 5 и 6, являются самыми популярными и по числу пользователей, и по числу тренировок? Есть ли виды спорта, популярные в одной метрике, но не в другой? | Интерпретируйте статистику по `duration` (раздел 7, `df.select('duration').toPandas().describe().T`). Какова медианная продолжительность тренировки? Что означает большая разница между 75-м перцентилем и максимумом? | Объясните бизнес-ценность анализа интервалов записи данных (`interval`, раздел 8). Почему компании Endomondo может быть важно знать, как часто их трекеры записывают данные? | **Напишите код PySpark**, чтобы вычислить среднюю продолжительность (`duration`) тренировок для пользователей мужского (`male`) и женского (`female`) пола отдельно.           | Предложите задачу бинарной классификации в Spark MLlib: предсказать, будет ли тренировка пользователя дольше медианной продолжительности для данного вида спорта. Какие признаки (пол пользователя, время старта, день недели - если бы был) могут быть использованы?                                              |
| 13      | Как вы думаете, почему в данных присутствует пол 'unknown' (раздел 3)? Предложите 2-3 возможные причины. | Сравните гистограммы `PerWorkoutRecordCount` для 'running' и 'indoor cycling' (раздел 5). Какие выводы можно сделать о типичной плотности записи данных для этих активностей? | Представьте, что Endomondo хочет запустить таргетированную кампанию для пользователей, занимающихся несколькими видами спорта. Какие данные из анализа в разделе 6 помогут сегментировать этих пользователей? | **Напишите код PySpark**, чтобы отобрать 10 случайных строк из датафрейма `df` (используйте `sample` или `limit` после `orderBy(rand())`).                            | Можно ли использовать Spark MLlib для построения рекомендательной системы (например, на основе коллаборативной фильтрации), которая предлагает пользователям виды спорта, популярные среди похожих на них пользователей? Какие данные (`userId`, `sport`, возможно, `duration`) для этого нужны?             |
| 14      | Изучите раздел 7, где создается столбец `duration`. Какие виды спорта, судя по гистограммам, имеют наибольшую среднюю продолжительность тренировок? Подтверждается ли это таблицей `describe()`? | В разделе 8 используется `combineByKey` для расчета статистики по интервалам. Объясните (без деталей кода), какую задачу решает этот шаг агрегации данных. | Какие потенциальные проблемы с качеством данных можно выявить, анализируя графики необработанного пульса (раздел 8)? (например, аномальные пики, пропуски) | **Напишите код PySpark**, чтобы найти минимальное значение в массиве `heart_rate` для каждой тренировки и сохранить его в новый столбец `min_heart_rate` (используйте `array_min`). | Как данные о минимальном и максимальном пульсе (`min_heart_rate`, `max_heart_rate`) могут быть использованы как признаки для модели Spark MLlib? Например, для предсказания уровня физической подготовки пользователя или для выявления аномальных тренировок.                                              |
| 15      | Опишите своими словами разницу между RDD и DataFrame в Spark (упоминается в разделе 8). Почему для некоторых задач может быть удобнее использовать RDD? | Проанализируйте диаграмму `interval_statistic_df` (раздел 8). Для каких видов спорта характерен наибольший разброс значений интервалов (высокий `stdev interval`)? Что это может означать? | Предложите 2-3 бизнес-идеи (новые фичи, маркетинговые акции), основанные на анализе времени начала тренировок (`workout_start_time`, раздел 8). | **Напишите код PySpark**, чтобы отсортировать датафрейм `df` по столбцу `duration` (предполагая, что он создан) в порядке убывания и показать топ-5 самых долгих тренировок. | Опишите, как можно использовать алгоритмы обнаружения аномалий из Spark MLlib (или статистические методы) для выявления тренировок с необычно большими интервалами записи данных (`interval`). Зачем это может быть нужно бизнесу?                                                                    |
| 16      | Какие основные библиотеки Python (кроме PySpark) используются в этой практической работе? Какова их роль в анализе данных? | Интерпретируйте результаты анализа пользователей, занимающихся более чем одним видом спорта (`user_more_sports_df`, раздел 6). Какой процент пользователей занимается более чем одним видом спорта (оценочно)? | Для анализа пульса (раздел 8) данные были сэмплированы. Какие ограничения на выводы накладывает использование выборки? | **Используя Python и Pandas**, после выполнения `result_pdf = df.select('sport').limit(100).toPandas()`, напишите код для подсчета количества каждой активности в `result_pdf`. | Какие шаги по предобработке данных (feature scaling, one-hot encoding) потребуются для подготовки категориальных (`gender`, `sport`) и числовых (`duration`) признаков к использованию в большинстве моделей Spark MLlib?                                                                             |
| 17      | Что означает параметр `mode="DROPMALFORMED"` при чтении JSON файла в разделе 1? Какие риски это несет для анализа данных? | Сравните распределение 'workout_start_time' для мужчин и женщин в рамках одного популярного вида спорта (например, 'running') по гистограммам из раздела 8. Есть ли заметные отличия? | Какие еще данные (которых нет в этом датасете) могли бы быть полезны для более глубокого анализа поведения пользователей Endomondo? Приведите 2-3 примера. | **Напишите код PySpark**, чтобы проверить, есть ли хотя бы одна тренировка по 'swimming' в датафрейме `df`, где значение 'gender' равно 'unknown'.                     | Если бы у вас были данные о погоде (температура, осадки), как бы вы интегрировали их в модель Spark MLlib для предсказания, например, продолжительности (`duration`) или вероятности отмены тренировки? Какие типы признаков это были бы (категориальные, числовые)?                                           |
| 18      | Опишите шаги, предпринятые в разделе 3 для первичной оценки данных (`user_activity_workout_summarize`). Почему важно получить эти общие цифры перед углубленным анализом? | Проанализируйте stacked bar chart гендерного распределения по видам спорта (раздел 6). Назовите 2-3 вида спорта с наиболее сбалансированным гендерным составом. | Интерпретируйте 3D-график для 'orienteering' (раздел 8). Что необычного в траектории этого вида спорта по сравнению, например, с 'running'? | **Напишите код PySpark**, чтобы создать новый датафрейм, содержащий только `userId` тех пользователей, которые занимались 'running' И 'cycling'.                          | Предложите идею ML-модели в Spark MLlib для прогнозирования оттока пользователей (churn prediction). Какие признаки, отражающие активность пользователя (частота тренировок, разнообразие видов спорта, динамика продолжительности), могли бы быть ключевыми? Какая метрика оценки модели была бы важна?           |
| 19      | Какая информация теряется при агрегации данных о времени тренировки до часа начала (`workout_start_time = hour(...)` в разделе 7)? В каких случаях эта потеря информации может быть критичной? | Изучите таблицу `top_activities_by_gender_df` (раздел 6). Какой вид спорта из топ-5 имеет наибольшую долю 'unknown' гендера? Какие проблемы это создает? | Объясните, как анализ данных о частоте пульса (`heart_rate`) может быть использован для персонализации пользовательского опыта в фитнес-приложении. | **Напишите код PySpark**, чтобы переименовать столбец `userId` в `user_identifier` в датафрейме `df`.                                                               | Как проблема с 'unknown' гендером может повлиять на построение моделей ML в Spark MLlib? Предложите 2 стратегии обработки этого признака перед обучением модели (например, удаление строк, imputation).                                                                                           |
| 20      | В разделе 1 упоминается сравнение производительности Spark и Pandas. В каких ситуациях, исходя из этого сравнения, использование Spark является предпочтительным для бизнес-аналитика? | Проанализируйте гистограммы `duration` для 'aerobics' и 'strength training' (раздел 7). Сравните их типичную продолжительность. | На основе 3D-графиков (раздел 8), можно ли предположить, какие тренировки проходили в городской среде, а какие - на природе? По каким признакам? | **Напишите код PySpark**, чтобы найти всех пользователей (`userId`), у которых есть хотя бы одна тренировка продолжительностью (`duration`) более 120 минут.              | Опишите, как можно использовать Spark MLlib для автоматического определения типа активности (`sport`) на основе данных сенсоров (скорость, пульс, изменение высоты), если бы метка `sport` отсутствовала или была неверной. Какая модель (например, дерево решений, случайный лес) могла бы подойти?           |
| 21      | Просмотрите код UDF `get_interval` (раздел 7). Что возвращает эта функция, если в тренировке была всего одна временная метка? Как это может повлиять на дальнейший анализ? | Интерпретируйте boxplot `Sports count by gender` (раздел 6). Что означают точки над "усами" диаграммы? Какую информацию они несут? | Предложите метрику для оценки "интенсивности" тренировки, которую можно было бы рассчитать на основе имеющихся данных (например, используя `heart_rate` и `duration`). | **Напишите код PySpark**, чтобы вычислить стандартное отклонение для столбца `duration` (предполагая, что он создан).                                                | Как можно использовать стандартное отклонение продолжительности тренировок (`duration`) для конкретного пользователя как признак в модели MLlib? Что может характеризовать этот признак (например, стабильность режима тренировок)?                                                                |
| 22      | Какие виды визуализации использовались в практической работе? Для каких типов данных или задач каждый вид визуализации подходит лучше всего? | Проанализируйте таблицу `interval_statistic_df` (раздел 8). Найдите вид спорта с минимальным средним интервалом (`mean interval`). Что это говорит о записи данных для этого спорта? | Как можно использовать данные о `longitude` и `latitude` для выявления популярных мест для тренировок в определенном городе (если бы были данные по одному городу)? | **Напишите код PySpark**, чтобы добавить столбец `is_long_workout`, который равен 1, если `duration` > 60, и 0 в противном случае (используйте `when` / `otherwise`).   | Опишите, как созданный бинарный признак `is_long_workout` может быть использован как целевая переменная для задачи классификации в Spark MLlib. Какую бизнес-задачу решает модель, предсказывающая, будет ли тренировка "длинной"?                                                                    |
| 23      | Оцените названия столбцов в исходном датасете (раздел 2). Насколько они понятны для бизнес-пользователя? Есть ли столбцы, требующие переименования или дополнительного описания? | В разделе 6 рассчитывается процент пользователей, занимающихся более чем 1 видом спорта. Как бы вы интерпретировали этот показатель для бизнеса? Это высокий или низкий процент? | Если бы у вас были данные о погоде во время тренировки, как бы вы могли их использовать совместно с данными о `duration` или `heart_rate`? | **Напишите код PySpark**, чтобы посчитать количество строк в датафрейме `df` до и после удаления дубликатов по столбцу `id` (уникальный ID тренировки).                 | Почему удаление дубликатов важно перед обучением моделей ML в Spark MLlib? Как дубликаты могут повлиять на качество модели и оценку ее производительности?                                                                                                                                 |
| 24      | Зачем в разделе 7 при конвертации времени из Unix timestamp вычиталось 7 часов (`timedelta(hours=7)`)? Почему важно учитывать часовые пояса при анализе временных данных? | Сравните `mean interval` и `50th percentile interval` (медиану) в таблице `interval_statistic_df` (раздел 8). Для каких видов спорта эти значения сильно различаются? Что это говорит о распределении интервалов? | Представьте, что бизнес хочет понять, как часто пользователи бросают тренировку на полпути. Какие из имеющихся данных могли бы косвенно указывать на это? | **Напишите код PySpark**, чтобы отобрать тренировки (`id` и `sport`), которые начались (`workout_start_time`) между 6 и 9 часами утра (включительно).                 | Как признак "утренняя тренировка" (например, бинарный флаг на основе `workout_start_time`) может быть использован в модели MLlib? Например, для предсказания типа активности или для сегментации пользователей ("жаворонки" vs "совы").                                                              |
| 25      | В разделе 1 код инициализирует SparkSession с определенными параметрами (`MAX_MEMORY`, `timeout`). Почему настройка этих параметров важна при работе с большими данными? | Проанализируйте график распределения `PerWorkoutRecordCount` (раздел 5). Видны ли пики на определенных значениях (например, 500)? Что это может означать? | Какие этические соображения должен учитывать бизнес-аналитик при работе с данными о местоположении (`latitude`, `longitude`) и биометрии (`heart_rate`) пользователей? | **Напишите код PySpark**, чтобы вычислить максимальное значение пульса (`heart_rate`) для каждой тренировки и сохранить его в новый столбец `max_heart_rate` (используйте `array_max`). | Как можно использовать Spark MLlib (например, алгоритм K-Means) для кластеризации пользователей на основе их средних показателей тренировок (например, `avg_heart_rate`, `duration`, `workout_start_time`)? Какие бизнес-сегменты пользователей можно было бы выявить таким образом?                           |
| 26      | Опишите процесс создания категориальной переменной 'others' при анализе топ-5 видов спорта (раздел 5). Зачем это делается и как влияет на интерпретацию диаграмм? | Изучите гистограммы `workout_start_time` (раздел 8). Есть ли виды спорта, которые чаще практикуют в рабочее время (например, с 9 до 17)? | Как можно использовать данные о `speed` (хотя детально не анализировались) в сочетании с `heart_rate` для оценки эффективности тренировки? | **Напишите код PySpark**, чтобы найти уникальные значения в столбце `sport` и вывести их в виде списка.                                                              | Перед использованием признака `sport` в большинстве моделей MLlib его нужно преобразовать (например, через `StringIndexer` и `OneHotEncoder`). Объясните, зачем нужны эти шаги.                                                                                                                |
| 27      | Какой столбец используется для уникальной идентификации каждой тренировки? А какой для уникальной идентификации пользователя? (Раздел 2) | В разделе 8 при построении графика `interval_statistic_df` используются одновременно bar chart и line chart. Объясните, почему выбрано такое сочетание для визуализации разных статистик. | Предложите способ визуализации данных о маршруте тренировки в 2D (используя только `longitude` и `latitude`), который мог бы быть альтернативой 3D-графику. | **Напишите код PySpark**, чтобы выбрать топ-10 пользователей (`userId`) по общему количеству тренировок.                                                              | Как информация о топ-пользователях (наиболее активных) может быть полезна при построении моделей ML в Spark MLlib? Например, при создании модели оценки вовлеченности или при выявлении лидеров мнений.                                                                                           |
| 28      | Как наличие столбца `url` (раздел 2) могло бы обогатить анализ, если бы данные по этим ссылкам были доступны? | Интерпретируйте значение 'stdev interval' для 'mountaineering' в таблице `interval_statistic_df` (раздел 8). Что означает высокое стандартное отклонение в данном контексте? | Если бы вы обнаружили, что у значительной части пользователей очень высокие или низкие значения `heart_rate` во время отдыха (начало/конец тренировки), какие выводы или дальнейшие шаги вы бы предложили? | **Напишите код PySpark**, чтобы отфильтровать `df`, оставив только тренировки, где вид спорта (`sport`) содержит слово 'bike' (используйте `like` или `contains`).        | Если бы вы хотели построить модель MLlib для различения 'cycling' и 'mountain biking' на основе данных сенсоров (скорость, высота, пульс), какие признаки были бы наиболее дискриминирующими? Какие преобразования признаков могли бы усилить различия?                                                   |
| 29      | В разделе 5 строится круговая диаграмма с параметром `explode`. Что делает этот параметр и зачем он используется при визуализации? | Проанализируйте гистограммы `duration` (раздел 7). Для каких видов спорта распределение сильно смещено вправо (есть "длинный хвост" очень долгих тренировок)? | Как можно было бы использовать данные `altitude` для классификации тренировок (например, "равнинная", "холмистая", "горная")? | **Напишите код PySpark**, чтобы создать столбец `avg_heart_rate` путем вычисления среднего значения массива `heart_rate` (может потребоваться UDF или `aggregate` / `expr`). | Опишите, как признак `avg_heart_rate` может быть использован в регрессионной модели Spark MLlib для предсказания сожженных калорий (если бы такие данные были). Почему средний пульс является хорошим показателем интенсивности?                                                                     |
| 30      | Практическая работа использует Google Colab и загружает данные из Google Drive / Yandex Disk. Какие преимущества и недостатки у такого подхода для бизнес-аналитика по сравнению с работой в корпоративной BI-системе? | Изучите код `sampling_data` (раздел 8). Как обеспечивается случайность выборки пользователей и тренировок? Почему это важно для репрезентативности выборки? | Предложите бизнес-метрику, которая бы отражала "регулярность" занятий спортом конкретным пользователем, используя столбец `timestamp` или `date_time`. | **Используя Python**, напишите функцию `check_interval(interval_list, threshold)`, которая принимает список интервалов и порог, и возвращает `True`, если *любой* интервал в списке превышает порог, иначе `False`. | Как показатель "регулярности" тренировок (например, количество тренировок за последнюю неделю/месяц) может быть использован как признак в модели MLlib для предсказания вероятности того, что пользователь продолжит использовать приложение в следующем месяце?                                             |

#### Альтернативная работа(необязательно, в случае, если есть готовый проект)

В датасете должны быть представлены табличные данные, желательный объем — от нескольких гигабайт и выше. Датасеты можно выбрать на следующих ресурсах:

- [kaggle](https://www.kaggle.com/datasets) (для скачивания датасета требуется регистрация);

- [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php).

8. Выполните разведочный анализ датасета с определением: 
* типов признаков в датасете; 
* пропущенных значений и их устранением; 
* выбросов и их устранением; 
* расчетом статистических показателей признаков (средних, квартилей и т.д.); 
* визуализацией распределения наиболее важных признаков; 
* корреляций между признаками.

9. Сделайте выводы по работе.
10. Сохраните ноутбук с проведенным анализом в `GitHub / GitLab`, в котором опишите постановку задачи, описание датасета со ссылкой на него, проведенный разведовательный анализ и выводы.

**К отчету** следует представить ссылку на репозиторий `GitHub / GitLab/Gitverse` с выполненным разведочным анализом, а также быть готовым продемонстрировать работоспособность кода и пояснить спорные моменты.


#### Литература для подготовки к отчету:
1. Изучаем Spark: молниеносный анализ данных / Х. Карау, Э. Конвински, П. Венделл, М.М. Захария // ДМК Пресс, 2015. — 304 с.: ил.
2. Data Exploration // Learning Apache Spark with Python [Электронный  ресурс] / W. Feng. - [2021]. - Режим доступа : https://runawayhorse001.github.io/LearningApacheSpark/exploration.html (дата обращ. 10.03.2024).
3. Advanced Pyspark for Exploratory Data Analysis [Электронный  ресурс]. – [2022]. – Режим доступа : https://www.kaggle.com/code/tientd95/advanced-pyspark-for-exploratory-data-analysis (дата обращ. 10.03.2024). 
4. Exploratory Data Analysis (EDA) with PySpark on Databricks [Электронный  ресурс]. – [2020]. – Режим доступа : https://towardsdatascience.com/exploratory-data-analysis-eda-with-pyspark-on-databricks-e8d6529626b1 (дата обращ. 10.03.2024).
5. Exploratory data analysis with pySpark [Электронный  ресурс]. – [2020]. – Режим доступа : https://github.com/BosenkoTM/PySpark/tree/main/pySpark_tutorial-master (дата обращ. 10.03.2024).
6. Официальный сайт Apache Spark [Электронный  ресурс]. – [2022]. – Режим доступа : https://spark.apache.org/ (дата обращ. 10.03.2024).
7. Уайт, Т. Hadoop: Подробное руководство / Т. Уайт. — 3-е изд. — СПб. : Питер, 2013. — 672 с.: ил.
